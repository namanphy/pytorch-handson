{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autograd-tensor-functions-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UvgBbQYx9Xso",
        "yZ6ATp5FLA8g"
      ],
      "authorship_tag": "ABX9TyNwof5630W47/oWD6rofJgM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namanphy/pytorch-handson/blob/master/autograd_tensor_functions_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ1M2Y3sn4_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpFUCVFQrqu4",
        "colab_type": "text"
      },
      "source": [
        "## Autograd\n",
        "It is a tool that does the calculation of derivatives via a technique called **automatic differentiation**. As quoted from official documentation : *`torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions*. \n",
        "\n",
        "Automatic differentiation is a set of techniques to numerically evaluate the derivative of a function. As it is recquired during the backpropagation pass(to compute the gradient of weights w.r.t loss function) while training a neural network.\n",
        "\n",
        "\n",
        "## Computation Graph\n",
        "So how does during backpropagation, pytorch(or any other DL library for that matter) calculates gradients, it does by generating a data structure called **Computation graph**. \n",
        "In a complex setup where there is thousands of variables to calculate gradient, a computation graph comes into picture.\n",
        "\n",
        "**Computation graph are nothing but a simple map of references of variables(or tensors) and operators(or functions) generated for a set of algebric equations, through which autograd can traverse and trace back to leafs) to calculate gradients.**\n",
        "\n",
        "Now, as pytorch generate these graphs during runtime in a forward pass(simple calculation of outputs from inputs), graphs are called Dynamic Computation Graphs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMrKYrnszAuQ",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we are going to discuss the following functions and properties assciated with tensors.\n",
        "\n",
        "**1.** 3 basic properties of tensors\n",
        "*   grad_fn : how to view the different fallback functions during backprop.\n",
        "*   requires_grad : \n",
        "*   is_leaf : If it is leaf in a graph.\n",
        "\n",
        "**2.** backprop() : \n",
        "\n",
        "**3.** retain_grad() : \n",
        "\n",
        "**4.** register_hook() : \n",
        "\n",
        "**5.** detatch() :\n",
        "  \n",
        "**6**  EXTRA - torch.no_grad() :\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvgBbQYx9Xso",
        "colab_type": "text"
      },
      "source": [
        "## 1. Important properties : requires_grad, grad_fn, is_leaf\n",
        "The `requires_grad` attribute tells autograd to track your operations. So if you want PyTorch to create a graph corresponding to these operations, you will have to set the `requires_grad` attribute of the Tensor to True.\n",
        "\n",
        "There are 2 ways in which it can be done, either by passing it as argument in `torch.tensor` or explicitly setting up the `requires_grad` property to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kLHhqVY-X1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e718f7f2-ed76-4f83-f0ce-718514eb101e"
      },
      "source": [
        "t0 = torch.tensor([1., 2.], requires_grad=True)\n",
        "print(f't0 : {t0}')\n",
        "\n",
        "t1 = torch.FloatTensor([1., 2.])  # dtype = torch.float32\n",
        "t1.requires_grad=True\n",
        "print(f't1 : {t1}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t0 : tensor([1., 2.], requires_grad=True)\n",
            "t1 : tensor([1., 2.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjzoOJMB0xei",
        "colab_type": "text"
      },
      "source": [
        "It is to remember that tensors with only `float` data types can require gradient (or ask `autograd` to record its operations).\n",
        "The other two `float` dtype tensors available in pytorch are :\n",
        "\n",
        "1. torch.HalfTensor (torch.float16)\n",
        "2. torch.DoublTensor (torch.float64)\n",
        "\n",
        "**It has a property :**\n",
        "  - The Tensors generated by applying any operations on other tensors, given that the for atleast one input tensor `requires_grad = True`, then the resultant tensor will also have `requires_grad = True`. \n",
        "  - It is also helpful when in a network we dont want to change the gradients and hence dont want to update the weights associated with some tensors. Just setting `requires_grad` to False the tensors won't participate in computation graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7b6A9W0S2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a694423a-ef16-4d85-9d25-3a2fdbb69482"
      },
      "source": [
        "t2 = torch.HalfTensor([1., 2.])  # dtype = torch.float16\n",
        "t2.requires_grad=True\n",
        "print(f't2 : {t2}')\n",
        "\n",
        "t3 = torch.DoubleTensor([1., 2.])  # dtype = torch.float64\n",
        "t3.requires_grad=True\n",
        "print(f't3 : {t3}')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t2 : tensor([1., 2.], dtype=torch.float16, requires_grad=True)\n",
            "t3 : tensor([1., 2.], dtype=torch.float64, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR8zlxsAHVwz",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84kdyKgi3eff",
        "colab_type": "text"
      },
      "source": [
        "The `grad_fn` property holds the reference to the function (mathematical operator) that creates it. It is very important during backward pass as the function here is responsible to calculate the gradient and pass it to appropiate next function in the pass.\n",
        "  - If `requires_grad` is set to False, `grad_fn` would be None.\n",
        "\n",
        "The `is_leaf` property tells whether a tensor is a leaf node or not. Essentially leaf tensors are the tensors whom we want to accumuate the gradient and are present at the edge of the computation graph. **Only leaf Tensors will have their grad populated during a call to** `backward()`. Technically, the `leaf tensors` are any tensors that created by following approaches :\n",
        "1. Tensors resulting in operations from tensors that have `requires_grad = False` will be leaf Tensors.\n",
        "\n",
        "2. Any tensor that is explicitly created by the user will be leaf Tensors.\n",
        "This means that they are not the result of an operation and so grad_fn is None.\n",
        "\n",
        "3. Obtained from `detach()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTDZPHbp9Vav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "082ad5f6-b5ec-4562-ab0c-69fa12eb8686"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "a = torch.tensor(4., requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "y = a * x\n",
        "\n",
        "z = y + b\n",
        "\n",
        "print(\"Tensor x\")\n",
        "print(f'grad funtion = {x.grad_fn}')\n",
        "print(f'is leaf = {x.is_leaf}')\n",
        "print(x.requires_grad)\n",
        "\n",
        "print(\"\\nTensor y\")\n",
        "print(f'grad funtion = {y.grad_fn}')\n",
        "print(f'is leaf = {y.is_leaf}')\n",
        "print(y.requires_grad)\n",
        "\n",
        "print(\"\\nTensor z\")\n",
        "print(f'grad funtion = {z.grad_fn}')\n",
        "print(f'is leaf = {z.is_leaf}')\n",
        "print(z.requires_grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor x\n",
            "grad funtion = None\n",
            "is leaf = True\n",
            "True\n",
            "\n",
            "Tensor y\n",
            "grad funtion = <MulBackward0 object at 0x7f63d5fd2ba8>\n",
            "is leaf = False\n",
            "True\n",
            "\n",
            "Tensor z\n",
            "grad funtion = <AddBackward0 object at 0x7f63d5fd2ba8>\n",
            "is leaf = False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ4qyoSFJVAO",
        "colab_type": "text"
      },
      "source": [
        "As in above example, the tensor `x` is only the leaf node. And as `x` is a leaf node, the `grad_fn` is None (as it is not obtained from any operations). \n",
        "\n",
        "The tensor `y` has `grad_fn` a multiplication operator, since `y` is obtained from multiplication of `a` and `x`.  Similarly the case for `z`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ6ATp5FLA8g",
        "colab_type": "text"
      },
      "source": [
        "## 2. backward(gradient=None, retain_graph=None, create_graph=False)\n",
        "\n",
        "This the most important of the tensor methods present here. It basically computes the gradient of current tensor w.r.t. graph leaves. It is responsible to calculate the gradient during a backward pass.\n",
        "\n",
        "*Note : `backward` function only calculates gradients by going over a already made backward graph. The backward graph is as discussed genrared during a forward pass only.*\n",
        "\n",
        "1. The backward function **takes a incoming gradient** from the part of the network in front of it.\n",
        "2. Then it **calculates the local gradient** at a particular tensor.\n",
        "3. Then it **multiply the local gradient to with incoming gradient**.\n",
        "4. Finally **pass the computed gradient to the tensor's inputs** by invoking the backward method of the `grad_fn` of their inputs or simply **save the gradient in `grad` property for leaf nodes**.\n",
        "\n",
        "Suppose in the above example, when calling `z.backward`. The grad_fn of `z` is `<AddBackward>`.\n",
        "1. The backward function of `<AddBackward>` takes a default input tensor as `torch.tensor([1.])`.\n",
        "2. Then it calculates gradient for `y` and `b`. For both the gradients will be `[1.]` as it a add fucntion.\n",
        "3. The gradient is multiplied with the incoming tensor i.e. `[1.] * [1.]`.\n",
        "4. Now for `b` the grad_fn is `None` so the gradient computed directly will get stored in `grad` property of tensor `b`. And for tensor `y` the backward function passes the gradient to its input tensor's `grad_fn` (i.e. `<MulBackward>` of `y` since it is formed after multiplication of `x` and `a`)\n",
        "5. Similarly the backward function will be called for `y`'s `<MulBackward>`.\n",
        "\n",
        "As noticed, the backward function is recursively called through out the graph as we backtrack. You can access the gradients by calling the `grad` attribute of Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ad1IjyQLkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "2f413279-9fa6-4ba9-f371-67d57818abfd"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "a = torch.tensor(4., requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "y = a * x\n",
        "\n",
        "z = y + b\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print(\"Tensor x\")\n",
        "print(f'grad funtion = {x.grad_fn}')\n",
        "\n",
        "print(\"\\nTensor a\")\n",
        "print(f'grad funtion = {a.grad_fn}')\n",
        "\n",
        "print(\"\\nTensor b\")\n",
        "print(f'grad funtion = {b.grad_fn}')\n",
        "\n",
        "print(\"\\nTensor y\")\n",
        "print(f'grad funtion = {y.grad_fn}')\n",
        "\n",
        "print(\"\\nTensor z\")\n",
        "print(f'grad funtion = {z.grad_fn}')\n",
        "print(\"\\n\")\n",
        "print('dz/dx:', x.grad) \n",
        "print('dz/da:', a.grad) \n",
        "print('dz/db:', b.grad) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor x\n",
            "grad funtion = None\n",
            "\n",
            "Tensor a\n",
            "grad funtion = None\n",
            "\n",
            "Tensor b\n",
            "grad funtion = None\n",
            "\n",
            "Tensor y\n",
            "grad funtion = <MulBackward0 object at 0x7f63f1d01be0>\n",
            "\n",
            "Tensor z\n",
            "grad funtion = <AddBackward0 object at 0x7f63f1d01be0>\n",
            "\n",
            "\n",
            "dz/dx: tensor(4.)\n",
            "dz/da: tensor(3.)\n",
            "dz/db: tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wey-q8QsZf0D",
        "colab_type": "text"
      },
      "source": [
        "### Calling backward on non-scaler tensor.\n",
        "\n",
        "For a vector-valued tensor, the backward function gives a Runtime error : `grad can be implicitly created only for scalar outputs`. \n",
        "\n",
        "This is beacuse for a non-scalar tensor a jacobian-vector is to be computed and then the `backward` expects incoming gradients as it's input (usually the gradient of the differentiated function w.r.t. corresponding tensors) . Hence the `backward` expects incoming gradient a Tensor of same size as the current tensor, then it'll able to backpropagate.\n",
        "\n",
        "So either you can pass the tensor of the same shape or simply change the size of the current tensor to `torch.Size([])` as expected by backward.\n",
        "\n",
        "*NOTE : If you'll pass non-ones tensor in backward, the gradients will get scaled accordingly.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBbHqZCeVQ0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9a035efb-f91b-4503-bc33-9358b39b4b0a"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "a = torch.tensor([4.,2.], requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "y = a * x\n",
        "z = y + b \n",
        "\n",
        "z.backward(torch.tensor([1.,1.])) # passing a gradient to backward.\n",
        "\n",
        "print('dz/dx:', x.grad)\n",
        "print('dz/da:', a.grad) \n",
        "print('dz/db:', b.grad) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz/dx: tensor(6.)\n",
            "dz/da: tensor([3., 3.])\n",
            "dz/db: tensor(2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXz0cEMcY310",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4d0676e2-b3de-4ed3-8dab-ac9d041b12dd"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "a = torch.tensor([4.,2.], requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "y = a * x\n",
        "z = y + b \n",
        "\n",
        "z = z.mean()\n",
        "z.backward()\n",
        "\n",
        "print('dz/dx:', x.grad)\n",
        "print('dz/da:', a.grad) \n",
        "print('dz/db:', b.grad) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz/dx: tensor(3.)\n",
            "dz/da: tensor([1.5000, 1.5000])\n",
            "dz/db: tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfG4pdN_cifZ",
        "colab_type": "text"
      },
      "source": [
        "### Calling backward twice\n",
        "\n",
        "The computation graph that was generated during runtime and that is used to calculate the gradient during a backward pass, is essentialy gets freed when you call `backward`.\n",
        "As the gradients are already computed after calling `backward`, the graph is destroyed and will form again when you again run a forward pass.\n",
        "\n",
        "Hence, if  you'll set `retain_graph = True` the graph will not be freed and you'll be able to backpropagate through the same graph again to calculate the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6tUAff7ch3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "a = torch.tensor(4., requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "y = a * x\n",
        "z = y + b\n",
        "\n",
        "z.backward(retain_graph=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoTmlMtrfWCD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c7b81edf-acaa-4b1a-99d2-d4a2de3f79ff"
      },
      "source": [
        "print('dz/dx:', x.grad)\n",
        "print('dz/da:', a.grad) \n",
        "print('dz/db:', b.grad) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz/dx: tensor(4.)\n",
            "dz/da: tensor(3.)\n",
            "dz/db: tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_xyze5rn_LZ",
        "colab_type": "text"
      },
      "source": [
        "## 3. retain_grad()\n",
        "\n",
        "This function allows a tensor that is not a leaf node to store the gradient that passes through it during backward pass. It can help to troubleshoot the flow of gradients in the graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te2h4KC7g_4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "65a8a42b-284e-48c9-daa1-8b3a99aa726d"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "a = torch.tensor(4., requires_grad=True)\n",
        "b = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "y = a * x + b\n",
        "y.retain_grad()\n",
        "\n",
        "c = torch.tensor(4., requires_grad=True)\n",
        "d = torch.tensor(4., requires_grad=True)\n",
        "\n",
        "w = c * y + d\n",
        "w.retain_grad()\n",
        "\n",
        "w.backward()\n",
        "\n",
        "print('dw/dx:', x.grad) \n",
        "print('dw/da:', a.grad) \n",
        "print('dw/db:', b.grad) \n",
        "print('dw/dy:', y.grad)\n",
        "print('dw/dc:', c.grad) \n",
        "print('dw/dd:', d.grad) \n",
        "print('dw/dw:', w.grad)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw/dx: tensor(16.)\n",
            "dw/da: tensor(12.)\n",
            "dw/db: tensor(4.)\n",
            "dw/dy: tensor(4.)\n",
            "dw/dc: tensor(17.)\n",
            "dw/dd: tensor(1.)\n",
            "dw/dw: tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP1jAhTRwKbv",
        "colab_type": "text"
      },
      "source": [
        "## 4. register_hook()\n",
        "\n",
        "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:\n",
        "\n",
        "`hook(grad) -> Tensor or None`\n",
        "\n",
        "So, **the hook can take the value of `grad` and can a return a new value or perform operations with the value.**\n",
        "This is best part, this can help to \n",
        "1. Modify the `grad` on the fly during a backward pass without waiting for the pass to be completed. This can influence our ways to calculate gradient in a graph.\n",
        "2. Debug the the code for flow of gradients in your graph. Identifying gradients at each step even for non-leaf nodes.\n",
        "\n",
        "Looking a example from the pytorch documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDjORHtwwYDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00de4e79-a5a7-4a83-efc5-a56f28a1a351"
      },
      "source": [
        "v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
        "h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
        "v.backward(torch.tensor([1., 2., 3.]))\n",
        "print(v.grad)\n",
        "h.remove()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 4., 6.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4eUijGDwONj",
        "colab_type": "text"
      },
      "source": [
        "## 5. detach()\n",
        "\n",
        "It can create a copy of a tensor that is not a part of the computation graph i.e detaches the Tensor from the graph that created it, making it a leaf.\n",
        "Both tensors will share the same memory, output tensor will be a leaf with `grad_fn = None` and `requires_grad = False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHBFuaIBn4Pq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "16c5da5a-d4f3-4399-99c3-92693e8bdebe"
      },
      "source": [
        "print(f'Original V : {v}')\n",
        "new = v.detach()\n",
        "new[0] = 5\n",
        "print(f'New detached V : {new}')\n",
        "print(f'New V : {v}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original V : tensor([0., 0., 0.], requires_grad=True)\n",
            "New detached V : tensor([5., 0., 0.])\n",
            "New V : tensor([5., 0., 0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}